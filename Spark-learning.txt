the table is generally divided into two types:
1)fact table :  The fact table generally records water, such as sales lists, etc., usually with the growth of time constantly expanding.... means large tables
2)dimension table: Dimension tables (small tables) generally refer to fixed, less variable tables, such as contacts, items, etc., the general data is limited.


=============================================
JOINS :
------
1) We did not specify that we wanted to do an “inner-join”, by default spark performs an inner-join if no join type is given.
2)Either you should skip the join type or the column name should be wrapped into scala Seq if have a join type.



===============================================
Broadcast hash join :
--------------------
1) Table needs to be broadcast less than  spark.sql.autoBroadcastJoinThreshold the configured value,
   default 10M (or add a broadcast join the hint)
2) Base table can not be broadcast, such as the left outer join, only broadcast the right table.

3) In broadcast hash join, copy of one of the join relations are being sent to all the worker nodes and it saves shuffling cost. This is useful when you are joining a large relation with a smaller one. It is also known as map-side join(associating worker nodes with mappers).

// Create the Execution Plan
fact_table = fact_table.join(broadcast(dimension_table), // Here's the magic!
                         fact_table.col("dimension_id") === dimension_table.col("id"))

================================================
Sort Merge Joins :
------------------

================================================
Triggers :
----------

trigger create mirco-batch with specified time interval.
===============================================
Adaptive Query executions :
---------------------------
1) selecting joins strategy runtime - either broadcast join or sort merge join
2) selecting no.of partitions - default 200
3) data skew problem

for this we need to globally enable - spar.sql.adaptive.enabled=true
================================================

Speculative execution in spark:
-------------------------------
to handle slow running tasks in spark.
to enable spark.speculation= true

===============================================
Spark memory management

https://www.linkedin.com/pulse/apache-spark-memory-management-deep-dive-deepak-rajak/
Reserve memory : if you don’t give Spark executor at least 1.5 * Reserved Memory = 450MB heap,
                 it will fail with “please use larger heap size” error message.

User memory : In Spark, the size of this memory pool can be calculated as (“Java Heap” – “Reserved Memory”) * (1.0 – spark.memory.fraction),
 which is by default equal to (“Java Heap” – 300MB) * 0.40. For example, with 4GB heap you would have 1518 MB of User Memory.
 And again, this is the User Memory and its completely up to you what would be stored in this RAM and how, Spark makes completely
 no accounting on what you do there and whether you respect this boundary or not.
 Not respecting this boundary in your code might cause OOM error.

================================================

repartition vs coalesce :

https://blog.knoldus.com/apache-spark-repartitioning-v-s-coalesce/

================================================

Narrow transformation — In Narrow transformation, all the elements that are required to compute the records in single partition live in the single partition of parent RDD. A limited subset of partition is used to calculate the result. Narrow transformations are the result of map(), filter().
Wide transformation — In wide transformation, all the elements that are required to compute the records in the single partition may live in many partitions of parent RDD. The partition may live in many partitions of parent RDD. Wide transformations are the result of groupbyKey and reducebyKey.

================================================